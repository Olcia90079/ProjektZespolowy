9b. Komputery i superkomputery


Superkomputery – tym mianem określa się komputery, które w danym okresie czasu są najszybsze w działaniu. Szybkość superkomputerów mierzy się w FLOPS (ang. FLoating point Operations Per Second), liczbą operacji zmiennopozycyjnych na sekundę. Początkowo jednostką była Mflops (MegaFLOPS – 10 6 flops), później Gflops (GigaFLOPS – 10 9 flops), obecnie Tflops (TeraFLOPS – 10 12 flops), czyli 1 000 000 000 000 operacji na sekundę, a już wkrótce będzie Pflops (PetaFLOPS – 10 15 flops). Za pierwsze superkomputery uważa się komputery zbudowane przez Control Data Corporation. Ich konstruktorem był Seymour Cray, powszechnie uznawany za ojca superkomputerów – były to maszyny CDC 6600 (3 Mflops), CDC 7600 (36 Mflops) i CDC Star-100 (100 Mflops). W 1972 roku Cray odszedł z CDC i założył firmę Cray Research, zajmującą się głównie budową superkomputerów. Do początków lat 90. komputery Cray zajmowały czołowe miejsca na liście superkomputerów. Przez ponad dwa lata najszybsza była japońska maszyna Earth Simulator, a obecnie (rok 2005) prym wiodą komputery budowane przez IBM. W czerwcu 2005 najszybszy był superkomputer Blue Gene/L (136-183 Tflops), zainstalowany w Lawrence Livermore National Laboratory w Livermore, będący wspólnym przedsięwzięciem IBM i National Nuclear Security Administration Departamentu Energetyki USA.

Pierwsze superkomputery (maszyny CDC) bazowały na szybkich procesorach. W latach 70. w superkomputerach stosowano procesory wektorowe, a w latach 80.- 90. zaczęto stosować w najszybszych maszynach równoległe systemy procesorów. Obecnie superkomputery są klasterami mikroprocesorów RISC (ang. Reduced Instruction Set Computing), takich jak PA-RISC (HP) czy PowerPC (Apple, IBM, Motorola). Na szybkość działania takich maszyn istotny wpływ ma użyta w tych maszynach hierarchiczna budowa pamięci, zapewniająca stały dopływ danych i instrukcji do procesorów. Wyzwaniem jest również chłodzenie takiej ilości procesorów oraz minimalizacja długości przewodów (ostatnio zaczęto stosować łączność bezprzewodową).

Systemy operacyjne w superkomputerach bazują na systemie Unix, natomiast do ich programowania stosuje się specjalne języki, wykorzystujące równoległą architekturę jednostki obliczeniowej. Jednymi z najpopularniejszych są odmiany języka Fortran, których kompilatory generują znacznie szybsze kody, niż kompilatory języków C czy C++. Testowanie szybkości działania superkomputerów odbywa się na wybranych przykładach (ang. benchmarks) zagadnień obliczeń algebraicznych Linpack.

Budowane są również superkomputery o specjalnym przeznaczeniu, np. Deep Blue do gry w szachy (pokonał Gary Kasparowa) czy Deep Crack służący do łamania szyfrów DES. Moc obliczeniową, porównywalną z superkomputerami, mają rozproszone instalacje komputerów (np. PC). Ocenia się, że na przykład poszukiwanie kolejnych dużych liczb pierwszych Mersenne’a w projekcie GIPS ma moc rzędu 18 Tflops, a system maszyn wyszukujących Google ma moc obliczeniową 100-300 Tflops.

Superkomputery są stosowane w przypadkach bardzo intensywnych obliczeń matematycznych, pojawiających się np. przy prognozowaniu pogody i w badaniach klimatów, modelowaniu molekularnym, symulacji zjawisk fizycznych, kryptoanalizie (m. in. przy łamaniu szyfrów). Jednym z najważniejszych ich zastosowań jest symulacja wybuchów broni jądrowej, co umożliwiło zaprzestanie prowadzenia prób z tą bronią w przyrodzie.